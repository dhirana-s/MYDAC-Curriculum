{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install vaderSentiment\n",
    "%pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw, datetime, requests, json, time, numpy as np, pandas as pd\n",
    "\n",
    "import nltk.sentiment.vader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "import seaborn as sns\n",
    "from IPython import display\n",
    "from pprint import pprint\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(client_id = open('cred/client_id.txt').read(),\n",
    "                     client_secret = open('cred/client_secret.txt').read(),\n",
    "                     user_agent = open('cred/user_agent.txt').read(),\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define reddit post URL\n",
    "reddit_post_url = \"https://www.reddit.com/r/electricvehicles/comments/1e7x13p/it_is_not_the_evs_that_are_lacking_in_the_us_its/\"\n",
    "\n",
    "# Fetch the post by URL\n",
    "submission = reddit.submission(url=reddit_post_url)\n",
    "\n",
    "# Print basic post details\n",
    "print(\"Title:\", submission.title)\n",
    "print(\"Author:\", submission.author)\n",
    "print(\"Score:\", submission.score)\n",
    "print(\"Number of Comments:\", submission.num_comments)\n",
    "print(\"Post Content:\", submission.selftext)\n",
    "print(\"URL:\", submission.url)\n",
    "\n",
    "# Fetch and print comments\n",
    "print(\"\\nComments:\")\n",
    "submission.comments.replace_more(limit=None)  # Replace \"More Comments\" placeholders\n",
    "for comment in submission.comments.list():\n",
    "    print(comment.body)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list to hold comment data\n",
    "comments_data = []\n",
    "\n",
    "# Fetch and store comments\n",
    "submission.comments.replace_more(limit=None)\n",
    "for comment in submission.comments.list():\n",
    "    comments_data.append({\n",
    "        'comment_id': comment.id,\n",
    "        'author': comment.author.name if comment.author else \"Deleted\",\n",
    "        'score': comment.score,\n",
    "        'comment_text': comment.body\n",
    "    })\n",
    "\n",
    "# Create a DataFrame\n",
    "comments_df = pd.DataFrame(comments_data)\n",
    "\n",
    "# Display basic info\n",
    "print(comments_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cleaning function\n",
    "def clean_comment(comment):\n",
    "    comment = comment.lower()  # Lowercase\n",
    "    comment = re.sub(r'http\\S+|www\\S+|https\\S+', '', comment)  # Remove URLs\n",
    "    comment = re.sub(r'[^a-z\\s]', '', comment)  # Remove special characters\n",
    "    # Tokenize comments into words\n",
    "tokenized_comments = [comment.split() for comment in comments_df['cleaned_comment']]\n",
    "# Identify bigrams and trigrams\n",
    "bigram = Phrases(tokenized_comments, min_count=5, threshold=10)  # Adjust thresholds as needed\n",
    "trigram = Phrases(bigram[tokenized_comments], threshold=10)\n",
    "# Apply Phraser for efficiency\n",
    "bigram_model = Phraser(bigram)\n",
    "trigram_model = Phraser(trigram)\n",
    "# Apply the bigram and trigram models to the tokenized comments\n",
    "comments_with_phrases = [trigram_model[bigram_model[comment]] for comment in tokenized_comments]\n",
    "# Example output\n",
    "print(\"Example tokenized comment with phrases:\", comments_with_phrases[0])\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply cleaning to the DataFrame\n",
    "comments_df['cleaned_comment'] = comments_df['comment_text'].apply(clean_comment)\n",
    "\n",
    "print(comments_df[['comment_text', 'cleaned_comment']].head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Analyze sentiment\n",
    "comments_df['sentiment_score'] = comments_df['cleaned_comment'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "comments_df['sentiment_label'] = comments_df['sentiment_score'].apply(\n",
    "    lambda x: 'positive' if x > 0 else ('negative' if x < 0 else 'neutral')\n",
    ")\n",
    "\n",
    "print(comments_df[['cleaned_comment', 'sentiment_score', 'sentiment_label']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identifying themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize comments for topic modeling\n",
    "tokenized_comments = [comment.split() for comment in comments_df['cleaned_comment']]\n",
    "dictionary = corpora.Dictionary(tokenized_comments)\n",
    "corpus = [dictionary.doc2bow(text) for text in tokenized_comments]\n",
    "\n",
    "# Train LDA model\n",
    "lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=15)\n",
    "\n",
    "# Display topics\n",
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.countplot(x='sentiment_label', data=comments_df, order=['positive', 'neutral', 'negative'])\n",
    "plt.title('Sentiment Distribution')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "all_comments = ' '.join(comments_df['cleaned_comment'])\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_comments)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Most Frequent Words')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
