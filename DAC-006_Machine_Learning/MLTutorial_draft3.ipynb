{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (from imbalanced-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (from imbalanced-learn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xgboost in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (2.1.2)\n",
      "Requirement already satisfied: numpy in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (from xgboost) (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (from xgboost) (1.14.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Goal: predict if a company goes bankrupt or not\n",
    "%pip install imbalanced-learn\n",
    "%pip install xgboost\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1     169\n",
      "Name: count, dtype: int64\n",
      "After SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1    5286\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Desktop\\coding\\MYDAC-Curriculum\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[1269   44]\n",
      " [  21   30]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1313\n",
      "           1       0.41      0.59      0.48        51\n",
      "\n",
      "    accuracy                           0.95      1364\n",
      "   macro avg       0.69      0.78      0.73      1364\n",
      "weighted avg       0.96      0.95      0.96      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df['Bankrupt?']\n",
    "x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Resample the training data\n",
    "x_train_smote, y_train_smote = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", y_train.value_counts())\n",
    "print(\"After SMOTE:\", pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "# Initialize individual models\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight={0: 10, 1: 1})\n",
    "lr = LogisticRegression(random_state=42, class_weight='balanced')\n",
    "xgb = XGBClassifier(random_state=42, scale_pos_weight=10)  # Weights for imbalance\n",
    "\n",
    "# Create a Voting Classifier with soft voting\n",
    "voting_clf = VotingClassifier(estimators=[('rf', rf), ('lr', lr), ('xgb', xgb)], voting='soft')\n",
    "\n",
    "# Fit the model\n",
    "voting_clf.fit(x_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to make some edits to increase recall.\n",
    "\n",
    "1. increase max number of iterations\n",
    "2. scale the features\n",
    "3. switch to a different solver\n",
    "4. check for multicollinearity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1     169\n",
      "Name: count, dtype: int64\n",
      "After SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1    5286\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1255   58]\n",
      " [  17   34]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      1313\n",
      "           1       0.37      0.67      0.48        51\n",
      "\n",
      "    accuracy                           0.95      1364\n",
      "   macro avg       0.68      0.81      0.72      1364\n",
      "weighted avg       0.96      0.95      0.95      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df['Bankrupt?']\n",
    "x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Resample the training data\n",
    "x_train_smote, y_train_smote = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", y_train.value_counts())\n",
    "print(\"After SMOTE:\", pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_smote)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Add the scaler\n",
    "    ('lr', LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000))\n",
    "])\n",
    "\n",
    "# Initialize individual models\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight={0: 10, 1: 1})\n",
    "lr = LogisticRegression(random_state=42, class_weight='balanced', solver='saga', max_iter=1000)\n",
    "xgb = XGBClassifier(random_state=42, scale_pos_weight=10)  # Weights for imbalance\n",
    "\n",
    "# Create a Voting Classifier with soft voting\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('lr', lr_pipeline),  # Use the pipeline with scaling\n",
    "    ('xgb', xgb)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the model\n",
    "voting_clf.fit(x_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there's higher recall but at the expense of precision tho. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1     169\n",
      "Name: count, dtype: int64\n",
      "After SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1    5286\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1271   42]\n",
      " [  19   32]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1313\n",
      "           1       0.43      0.63      0.51        51\n",
      "\n",
      "    accuracy                           0.96      1364\n",
      "   macro avg       0.71      0.80      0.74      1364\n",
      "weighted avg       0.96      0.96      0.96      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df['Bankrupt?']\n",
    "x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Resample the training data\n",
    "x_train_smote, y_train_smote = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", y_train.value_counts())\n",
    "print(\"After SMOTE:\", pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_smote)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Add the scaler\n",
    "])\n",
    "\n",
    "# Initialize individual models\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight={0: 10, 1: 1})\n",
    "xgb = XGBClassifier(random_state=42, scale_pos_weight=10)  # Weights for imbalance\n",
    "\n",
    "# Create a Voting Classifier with soft voting\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('xgb', xgb)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the model\n",
    "voting_clf.fit(x_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1     169\n",
      "Name: count, dtype: int64\n",
      "After SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1    5286\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1274   39]\n",
      " [  19   32]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1313\n",
      "           1       0.45      0.63      0.52        51\n",
      "\n",
      "    accuracy                           0.96      1364\n",
      "   macro avg       0.72      0.80      0.75      1364\n",
      "weighted avg       0.97      0.96      0.96      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df['Bankrupt?']\n",
    "x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Resample the training data\n",
    "x_train_smote, y_train_smote = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", y_train.value_counts())\n",
    "print(\"After SMOTE:\", pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_smote)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Add the scaler\n",
    "])\n",
    "\n",
    "# Initialize individual models\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight={0: 5, 1: 3})\n",
    "xgb = XGBClassifier(random_state=42, scale_pos_weight=8)  # Weights for imbalance\n",
    "\n",
    "# Create a Voting Classifier with soft voting\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('xgb', xgb)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the model\n",
    "voting_clf.fit(x_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now time to tune the class_weight for RandomForestClassifier. So imma just test out different weight ratios bw the majority and minority classes. Once my precision is above 50% im packing up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1     169\n",
      "Name: count, dtype: int64\n",
      "After SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1    5286\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1275   38]\n",
      " [  18   33]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      1313\n",
      "           1       0.46      0.65      0.54        51\n",
      "\n",
      "    accuracy                           0.96      1364\n",
      "   macro avg       0.73      0.81      0.76      1364\n",
      "weighted avg       0.97      0.96      0.96      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df['Bankrupt?']\n",
    "x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "\n",
    "# Resample the training data\n",
    "x_train_smote, y_train_smote = smote.fit_resample(x_train, y_train)\n",
    "\n",
    "print(\"Before SMOTE:\", y_train.value_counts())\n",
    "print(\"After SMOTE:\", pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_smote)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "lr_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),  # Add the scaler\n",
    "])\n",
    "\n",
    "# Initialize individual models\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight={0: 5, 1: 1})\n",
    "xgb = XGBClassifier(random_state=42, scale_pos_weight=8)  # Weights for imbalance\n",
    "\n",
    "# Create a Voting Classifier with soft voting\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('rf', rf),\n",
    "    ('xgb', xgb)\n",
    "], voting='soft')\n",
    "\n",
    "# Fit the model\n",
    "voting_clf.fit(x_train_smote, y_train_smote)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = voting_clf.predict(x_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mild improvement in precision and recall!! Lets try again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Desktop\\coding\\MYDAC-Curriculum\\.venv\\Lib\\site-packages\\numpy\\ma\\core.py:2881: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'rf__n_estimators': 300, 'rf__min_samples_split': 2, 'rf__min_samples_leaf': 1, 'rf__max_depth': None, 'rf__class_weight': {0: 10, 1: 1}}\n",
      "Before SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1     169\n",
      "Name: count, dtype: int64\n",
      "After SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1    5286\n",
      "Name: count, dtype: int64\n",
      "Cross-validation scores: [0.58563074 0.50250627 0.6031746  0.54991646 0.57362155 0.55576441\n",
      " 0.61518379 0.60902256 0.53205931 0.60891813 0.63888889 0.62113617\n",
      " 0.54991646 0.53205931 0.68013784 0.5734127  0.57957393 0.53811612\n",
      " 0.50845865 0.5677736  0.61518379 0.57957393 0.63272765 0.62698413\n",
      " 0.57957393 0.54396408 0.53226817 0.57957393 0.6031746  0.59116541]\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1252   61]\n",
      " [  13   38]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.95      0.97      1313\n",
      "           1       0.38      0.75      0.51        51\n",
      "\n",
      "    accuracy                           0.95      1364\n",
      "   macro avg       0.69      0.85      0.74      1364\n",
      "weighted avg       0.97      0.95      0.95      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "y = df['Bankrupt?']\n",
    "x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'rf__n_estimators': [100, 300, 500],  # Number of trees\n",
    "    'rf__min_samples_split': [2, 5, 10],   # Minimum samples required to split a node\n",
    "    'rf__min_samples_leaf': [1, 2, 4],     # Minimum samples required at leaf node\n",
    "    'rf__max_depth': [None, 10, 30],   # Maximum depth of trees\n",
    "    'rf__class_weight': ['balanced', {0: 10, 1: 1}, {0: 5, 1: 3}]  # Class weights\n",
    "}\n",
    "\n",
    "# Create a pipeline that first applies SMOTE, then trains Random Forest\n",
    "pipeline = Pipeline([\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('rf', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "# Set up RandomizedSearchCV with recall as the scoring metric\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=30,  # Number of random combinations to test\n",
    "    cv=3,  # Number of cross-validation splits\n",
    "    n_jobs=-1,  # Use all available CPU cores\n",
    "    scoring='recall', \n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model using RandomizedSearchCV\n",
    "random_search.fit(x_train, y_train)\n",
    "\n",
    "# Print the best parameters found by RandomizedSearchCV\n",
    "print(f'Best parameters: {random_search.best_params_}')\n",
    "\n",
    "# Get the best model found by RandomizedSearchCV\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(x_test)\n",
    "\n",
    "cv_scores = random_search.cv_results_['mean_test_score']\n",
    "print(f'Cross-validation scores: {cv_scores}')\n",
    "\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net Income to Stockholder's Equity         0.044497\n",
      "Borrowing dependency                       0.041840\n",
      "Total debt/Total net worth                 0.038789\n",
      "Net Value Growth Rate                      0.032728\n",
      "Persistent EPS in the Last Four Seasons    0.032077\n",
      "                                             ...   \n",
      "Working Capital to Total Assets            0.003941\n",
      "Total Asset Turnover                       0.003872\n",
      "Operating Gross Margin                     0.003788\n",
      "Liability-Assets Flag                      0.000000\n",
      "Net Income Flag                            0.000000\n",
      "Length: 95, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "best_rf = best_model.named_steps['rf']\n",
    "feature_importances = pd.Series(best_rf.feature_importances_, index=x.columns)\n",
    "print(feature_importances.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok back to the intial one above that has a good balance and trying from this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
