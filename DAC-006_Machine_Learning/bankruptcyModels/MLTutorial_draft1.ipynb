{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Goal: predict if a company goes bankrupt or not\n",
    "%pip install imbalanced-learn\n",
    "%pip install xgboost\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import imbalanced-learn as imblearn\n",
    "from scipy.stats import kurtosis, skew\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sig no. of outliers observed for most columns - via dataWrangler. Since this is financial data, check for long-tailed distribution aka tail extends further than normal distribution. If there is no long-tailed distribution, we can apply Random Forest without log transformation to account for skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     Skewness     Kurtosis\n",
      "Bankrupt?                                            5.295388    26.028793\n",
      " ROA(C) before interest and depreciation before...  -0.323941     6.385205\n",
      " ROA(A) before interest and % after tax             -1.033727     9.031279\n",
      " ROA(B) before interest and depreciation after tax  -0.763564     7.922381\n",
      " Operating Gross Margin                             -8.043368   365.271816\n",
      "...                                                       ...          ...\n",
      " Liability to Equity                                27.459467  1208.316151\n",
      " Degree of Financial Leverage (DFL)                 45.724197  2473.617797\n",
      " Interest Coverage Ratio (Interest expense to E... -13.939528   911.131588\n",
      " Net Income Flag                                     0.000000          NaN\n",
      " Equity to Liability                                 7.401101    93.996331\n",
      "\n",
      "[96 rows x 2 columns]\n",
      "Skewed Features:\n",
      "                                                     Skewness     Kurtosis\n",
      "Bankrupt?                                            5.295388    26.028793\n",
      " ROA(A) before interest and % after tax             -1.033727     9.031279\n",
      " Operating Gross Margin                             -8.043368   365.271816\n",
      " Realized Sales Gross Margin                        -8.066572   366.771873\n",
      " Operating Profit Rate                             -70.237164  5206.265691\n",
      "...                                                       ...          ...\n",
      " Net Income to Stockholder's Equity                -37.964701  1943.282633\n",
      " Liability to Equity                                27.459467  1208.316151\n",
      " Degree of Financial Leverage (DFL)                 45.724197  2473.617797\n",
      " Interest Coverage Ratio (Interest expense to E... -13.939528   911.131588\n",
      " Equity to Liability                                 7.401101    93.996331\n",
      "\n",
      "[82 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Desktop\\coding\\MYDAC-Curriculum\\.venv\\Lib\\site-packages\\pandas\\core\\apply.py:1081: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  results[i] = self.func(v, *self.args, **self.kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Calculate skewness and kurtosis for each feature\n",
    "skewness = df.skew()\n",
    "kurtosis_values = df.apply(kurtosis)\n",
    "\n",
    "# Print skewness and kurtosis summary\n",
    "summary = pd.DataFrame({'Skewness': skewness, 'Kurtosis': kurtosis_values})\n",
    "print(summary)\n",
    "\n",
    "# Filter for skewed features (consider skewness > 1 or < -1 as skewed)\n",
    "skewed_features = summary[summary['Skewness'].abs() > 1]\n",
    "print(\"Skewed Features:\")\n",
    "print(skewed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This isn't working so gonna try a diff method results unreliable according to vscode due to many identical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Skewness     Kurtosis\n",
      "Fixed Assets to Assets                   82.577237  6814.000147\n",
      "Current Ratio                            82.577237  6814.000147\n",
      "Total income/Total expense               82.332424  6786.903523\n",
      "Net Value Growth Rate                    80.291844  6540.116467\n",
      "Contingent liabilities/Net worth         79.670620  6487.125425\n",
      "Realized Sales Gross Profit Growth Rate  77.925109  6291.000429\n",
      "Continuous Net Profit Growth Rate        67.097534  5392.615103\n",
      "Total Asset Return Growth Rate Ratio     62.499961  5071.235869\n",
      "Revenue per person                       59.434480  3568.408258\n",
      "Quick Assets/Current Liability           47.947300  2305.178322\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Method 2: accounting for huge no. of columns\n",
    "df_non_constant = df.loc[:, df.nunique() > 1] #filter out constant/nearly constant features to avoid precision loss issue\n",
    "\n",
    "# Calculate skewness and kurtosis for each feature\n",
    "skewness = df_non_constant.skew()\n",
    "kurtosis_values = df_non_constant.apply(kurtosis)\n",
    "\n",
    "summary = pd.DataFrame({'Skewness': skewness, 'Kurtosis': kurtosis_values}) # Create a summary DataFrame with both skewness and kurtosis\n",
    "sorted_summary = summary.sort_values(by=['Skewness', 'Kurtosis'], ascending=False) # Sort by Skewness and Kurtosis in desc order (most problematic first)\n",
    "print(sorted_summary.head(10))  # Shows top 10 most problematic columns based on skewness and kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's kurtosis in the 6000s (should be around 3) and skewness in 80s (should be around 1) its very high even for financial data from a good source cos it suggests heavy-tailed distributions far from normality and extreme outliers. could be outliers but i googled and saw the years in the data are the 1997 asian financial crisis, 2001 recession, 2008 global recession Taiwan faced. But tbh the high skewness in financial data makes sense cos of the nature of how assets and liabilities behave in real world aka right skewed distribution since small no. of firms will have most of market value/revenue. Also bankruptcies are rare events that the model must capture.\n",
    "\n",
    "So things to do:\n",
    "1. find the key features w extreme values/skewness by reviewing feature importance\n",
    "2. apply log transformation to features with high skewness\n",
    "3. use tree-based model random forst which handles outliers and skewed data better\n",
    "4. Keep in mind that your target variable (bankruptcy) is likely highly imbalanced (i.e., most companies won't go bankrupt). This imbalance itself can introduce skewness into your dataset. You may need to use techniques like SMOTE (Synthetic Minority Over-sampling Technique) or class weighting in your model to account for the imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9655\n",
      "Confusion Matrix:\n",
      "[[1310    3]\n",
      " [  44    7]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      1313\n",
      "           1       0.70      0.14      0.23        51\n",
      "\n",
      "    accuracy                           0.97      1364\n",
      "   macro avg       0.83      0.57      0.61      1364\n",
      "weighted avg       0.96      0.97      0.95      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df['Bankrupt?']\n",
    "x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler() #initialise scaler to bring it to normal distribution\n",
    "x_train_scaled = scaler.fit_transform(x_train) #fit scaler on training data and transform it\n",
    "x_test_scaled = scaler.transform(x_test) #transform test data using same scaler\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=42) # Initialize the model\n",
    "model.fit(x_train_scaled, y_train) # Fit the model to the training data\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test_scaled) #get the trained model to make predictions on test set\n",
    "\n",
    "\n",
    "# Now we can evaluate how well the model did.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification report (precision, recall, f1-score)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So accuracy is noice. But a bit misleading since got very imbalanced classes (ie. lot more safe than bankrupty firms). The model is just predicting the majority safe class correctly.\n",
    "\n",
    "Problems:\n",
    "1. there are 44 firms predicted to be safe but were actually bankrupt.\n",
    "2. only 70% precision for class 1 - meaning only 70% of firms predicted to be bankrupt were actually bankrupt\n",
    "3. only 14% recall for class 1 - meaning only 14% of actually bankrupt firms were correctly predicted as bankrupt. very bad cos the model is failing to identify many bankrupt companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try again.\n",
    "#add in the class_weight='balanced' to the model and see how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9670\n",
      "Confusion Matrix:\n",
      "[[1309    4]\n",
      " [  41   10]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      1313\n",
      "           1       0.71      0.20      0.31        51\n",
      "\n",
      "    accuracy                           0.97      1364\n",
      "   macro avg       0.84      0.60      0.65      1364\n",
      "weighted avg       0.96      0.97      0.96      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df['Bankrupt?']\n",
    "x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler() #initialise scaler to bring it to normal distribution\n",
    "x_train_scaled = scaler.fit_transform(x_train) #fit scaler on training data and transform it\n",
    "x_test_scaled = scaler.transform(x_test) #transform test data using same scaler\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10, class_weight='balanced', random_state=42) # Initialize the model\n",
    "model.fit(x_train_scaled, y_train) # Fit the model to the training data\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test_scaled) #get the trained model to make predictions on test set\n",
    "\n",
    "\n",
    "# Now we can evaluate how well the model did.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification report (precision, recall, f1-score)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are small improvements noice. let's test out SMOTE. Random Forest doesn't need standard scaling so just gonna remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1     169\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "After SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1    5286\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy: 0.9531\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1272   41]\n",
      " [  23   28]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1313\n",
      "           1       0.41      0.55      0.47        51\n",
      "\n",
      "    accuracy                           0.95      1364\n",
      "   macro avg       0.69      0.76      0.72      1364\n",
      "weighted avg       0.96      0.95      0.96      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df['Bankrupt?']\n",
    "x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "smote = SMOTE(random_state=42) #initialise SMOTE\n",
    "x_train_smote, y_train_smote = smote.fit_resample(x_train, y_train)\n",
    "print(\"Before SMOTE:\", y_train.value_counts())\n",
    "print(\"\\n\\nAfter SMOTE:\", pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42) # Initialize Random Forest\n",
    "model.fit(x_train_smote, y_train_smote) # fit model to trained data\n",
    "\n",
    "y_pred = model.predict(x_test) # make predictions on the test set\n",
    "\n",
    "# Accuracy of model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'\\nAccuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok So when we did SMOTE, some things happened.\n",
    "\n",
    "1. The precision of class 1 dropped -> from 70% to only 44% of firms predicted to be bankrupt were actually bankrupt.\n",
    "2. Recall of class 1 increased -> from 14%, its not 55% of actually bankrupt firms being correctly predicted to be bankrupt. \n",
    "\n",
    "Since its 2024, and people are becoming more protectionist, imma just assume that government will want to prioritise stability over efficiency. Cos missing even one bankrupt firm can be very bad for the country. So I'll focus on high recall first then try to maximise precision as much as possible.\n",
    "\n",
    "Let's try adding ENN to the pre-existing SMOTE to see if it improved precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9599\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1923   45]\n",
      " [  37   41]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1968\n",
      "           1       0.48      0.53      0.50        78\n",
      "\n",
      "    accuracy                           0.96      2046\n",
      "   macro avg       0.73      0.75      0.74      2046\n",
      "weighted avg       0.96      0.96      0.96      2046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df['Bankrupt?']\n",
    "x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)  # Oversample minority class\n",
    "enn = EditedNearestNeighbours(n_neighbors=3)  # Remove noisy samples based on k=3 neighbors\n",
    "\n",
    "smote_enn_pipeline = Pipeline([\n",
    "    ('smote', smote),\n",
    "    ('enn', enn)\n",
    "]) #combine SMOTE and ENN using a pipeline\n",
    "\n",
    "x_resampled, y_resampled = smote_enn_pipeline.fit_resample(x_train, y_train) #resample the training data\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(x_resampled, y_resampled)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Accuracy of model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mild improvement in recall without sacrificing precision.\n",
    "\n",
    "But ultimately, there is still a strong class imbalance cos the precision and recall for class 1 is still relatively low. Let's try some other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9599\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1923   45]\n",
      " [  37   41]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98      1968\n",
      "           1       0.48      0.53      0.50        78\n",
      "\n",
      "    accuracy                           0.96      2046\n",
      "   macro avg       0.73      0.75      0.74      2046\n",
      "weighted avg       0.96      0.96      0.96      2046\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df['Bankrupt?']\n",
    "x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority', random_state=42)  # Oversample minority class\n",
    "enn = EditedNearestNeighbours(n_neighbors=3)  # Remove noisy samples based on k=3 neighbors\n",
    "\n",
    "smote_enn_pipeline = Pipeline([\n",
    "    ('smote', smote),\n",
    "    ('enn', enn)\n",
    "]) #combine SMOTE and ENN using a pipeline\n",
    "\n",
    "x_resampled, y_resampled = smote_enn_pipeline.fit_resample(x_train, y_train) #resample the training data\n",
    "\n",
    "scale_pos_weight = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(x_resampled, y_resampled)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Accuracy of model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
