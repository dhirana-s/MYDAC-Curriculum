{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install imbalanced-learn\n",
    "%pip install xgboost\n",
    "%pip install shap\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from xgboost import XGBClassifier\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import textwrap\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score, recall_score, precision_score, f1_score, average_precision_score, precision_recall_curve, make_scorer, auc\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import kurtosis, skew\n",
    "from imblearn.under_sampling import EditedNearestNeighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actual Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = df.drop('Bankrupt?', axis=1)  # Features\n",
    "y = df['Bankrupt?']  # Target (Bankrupt: 0 or 1)\n",
    "\n",
    "# Handle missing values\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X[X > 1e10] = np.nan\n",
    "X[X < -1e10] = np.nan\n",
    "\n",
    "# Impute missing values with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Apply SMOTE to oversample the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_imputed, y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
    "\n",
    "# Define models\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "# Create a Voting Classifier to combine both models\n",
    "voting_model = VotingClassifier(estimators=[\n",
    "    ('rf', rf_model),\n",
    "    ('xgb', xgb_model)\n",
    "], voting='soft')\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),      # Standardize features\n",
    "    ('voting_model', voting_model)     # Use the voting model (ensemble)\n",
    "])\n",
    "\n",
    "# --- Cross-validation ---\n",
    "# Set up StratifiedKFold for cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the model pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),      # Standardize features\n",
    "    ('voting_model', voting_model)     # Use the voting model (ensemble)\n",
    "])\n",
    "\n",
    "# Perform cross-validation using ROC AUC as the evaluation metric\n",
    "roc_auc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Apply cross-validation and calculate the ROC AUC scores for each fold\n",
    "cv_scores = cross_val_score(pipeline, X_res, y_res, cv=cv, scoring=roc_auc_scorer)\n",
    "\n",
    "# Print out the cross-validation results\n",
    "print(\"Cross-validation ROC AUC scores: \", cv_scores)\n",
    "print(\"Mean ROC AUC score: \", cv_scores.mean())\n",
    "\n",
    "# --- Train the pipeline ---\n",
    "# Fit the pipeline first\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- Feature Importance from Random Forest ---\n",
    "# Access Random Forest model inside the Voting Classifier (after fitting)\n",
    "rf_model = pipeline.named_steps['voting_model'].named_estimators_['rf']\n",
    "\n",
    "# Extract feature importance\n",
    "rf_feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Feature names and importance as a dataframe\n",
    "feat_names = X.columns  # This should be defined before the feature importance step\n",
    "feat_importance_df = pd.DataFrame({\n",
    "    'Feature': feat_names,\n",
    "    'Importance': rf_feature_importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Show the top N features (e.g., top 20)\n",
    "top_n = 20\n",
    "top_features = feat_importance_df.head(top_n)\n",
    "\n",
    "# Visualize Random Forest feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')  # Correcting the barh command\n",
    "plt.title(f'Top {top_n} Feature Importances (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance on top\n",
    "plt.show()\n",
    "\n",
    "# --- Feature Importance from XGBoost using SHAP ---\n",
    "# Access XGBoost model inside the Voting Classifier (after fitting)\n",
    "xgb_model = pipeline.named_steps['voting_model'].named_estimators_['xgb']\n",
    "\n",
    "# Use SHAP for XGBoost feature importance\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Visualize SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_train, feature_names=feat_names)\n",
    "\n",
    "# --- Evaluate the Model ---\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# --- Classification Report ---\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# --- ROC-AUC Curve ---\n",
    "y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# --- Precision-Recall Curve ---\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Bankrupt', 'Bankrupt'], yticklabels=['Non-Bankrupt', 'Bankrupt'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = df.drop('Bankrupt?', axis=1)  # Features\n",
    "y = df['Bankrupt?']  # Target (Bankrupt: 0 or 1)\n",
    "\n",
    "# Handle missing values\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X[X > 1e10] = np.nan\n",
    "X[X < -1e10] = np.nan\n",
    "\n",
    "# Impute missing values with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Apply SMOTE to oversample the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_imputed, y)\n",
    "\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "x_temp, x_val, y_temp, y_val = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    ")  # Reserve 20% for validation\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")  # 60% training, 20% testing (since 0.25 of 80% = 20%)\n",
    "\n",
    "\n",
    "# Define models\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "\n",
    "# Create a Voting Classifier to combine both models\n",
    "voting_model = VotingClassifier(estimators=[\n",
    "    ('rf', rf_model),\n",
    "    ('xgb', xgb_model)\n",
    "], voting='soft')\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),      # Standardize features\n",
    "    ('voting_model', voting_model)     # Use the voting model (ensemble)\n",
    "])\n",
    "\n",
    "# Train the pipeline with the training data\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = pipeline.predict(x_val)\n",
    "y_val_proba = pipeline.predict_proba(x_val)[:, 1]\n",
    "\n",
    "# Calculate metrics for validation data\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nValidation Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "# Calculate ROC-AUC for validation data\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "print(f\"Validation ROC-AUC Score: {roc_auc_val:.4f}\")\n",
    "\n",
    "# Plot ROC Curve for validation data\n",
    "fpr_val, tpr_val, thresholds_val = roc_curve(y_val, y_val_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_val, tpr_val, color='g', label=f'Validation ROC curve (area = {roc_auc_val:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random classifier line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('Validation ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = pipeline.predict(x_test)\n",
    "y_proba = pipeline.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Calculate ROC-AUC score for the test data\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"Test ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC Curve for test data\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_test, tpr_test, color='b', label=f'Test ROC curve (area = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random classifier line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('Test ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Cross-validation ---\n",
    "# Set up StratifiedKFold for cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the model pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),      # Standardize features\n",
    "    ('voting_model', voting_model)     # Use the voting model (ensemble)\n",
    "])\n",
    "\n",
    "# Perform cross-validation using ROC AUC as the evaluation metric\n",
    "roc_auc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Apply cross-validation and calculate the ROC AUC scores for each fold\n",
    "cv_scores = cross_val_score(pipeline, X_res, y_res, cv=cv, scoring=roc_auc_scorer)\n",
    "\n",
    "# Print out the cross-validation results\n",
    "print(\"Cross-validation ROC AUC scores: \", cv_scores)\n",
    "print(\"Mean ROC AUC score: \", cv_scores.mean())\n",
    "\n",
    "# --- Train the pipeline ---\n",
    "# Fit the pipeline first\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- Feature Importance from Random Forest ---\n",
    "# Access Random Forest model inside the Voting Classifier (after fitting)\n",
    "rf_model = pipeline.named_steps['voting_model'].named_estimators_['rf']\n",
    "\n",
    "# Extract feature importance\n",
    "rf_feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Feature names and importance as a dataframe\n",
    "feat_names = X.columns  # This should be defined before the feature importance step\n",
    "feat_importance_df = pd.DataFrame({\n",
    "    'Feature': feat_names,\n",
    "    'Importance': rf_feature_importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Show the top N features (e.g., top 20)\n",
    "top_n = 20\n",
    "top_features = feat_importance_df.head(top_n)\n",
    "\n",
    "# Visualize Random Forest feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')  # Correcting the barh command\n",
    "plt.title(f'Top {top_n} Feature Importances (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance on top\n",
    "plt.show()\n",
    "\n",
    "# --- Feature Importance from XGBoost using SHAP ---\n",
    "# Access XGBoost model inside the Voting Classifier (after fitting)\n",
    "xgb_model = pipeline.named_steps['voting_model'].named_estimators_['xgb']\n",
    "\n",
    "# Use SHAP for XGBoost feature importance\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Visualize SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_train, feature_names=feat_names)\n",
    "\n",
    "# --- Evaluate the Model ---\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# --- Classification Report ---\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# --- ROC-AUC Curve ---\n",
    "y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# --- Precision-Recall Curve ---\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Bankrupt', 'Bankrupt'], yticklabels=['Non-Bankrupt', 'Bankrupt'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimise the code above and structure properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = df.drop('Bankrupt?', axis=1)  # Features\n",
    "y = df['Bankrupt?']  # Target (Bankrupt: 0 or 1)\n",
    "\n",
    "# Handle missing values\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X[X > 1e10] = np.nan\n",
    "X[X < -1e10] = np.nan\n",
    "\n",
    "# Impute missing values with the median\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "\n",
    "# Apply SMOTE to oversample the minority class\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_imputed, y)\n",
    "\n",
    "\n",
    "# Split the data into training, validation, and testing sets\n",
    "x_temp, x_val, y_temp, y_val = train_test_split(\n",
    "    X_res, y_res, test_size=0.2, random_state=42, stratify=y_res\n",
    ")  # Reserve 20% for validation\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp\n",
    ")  # 60% training, 20% testing (since 0.25 of 80% = 20%)\n",
    "\n",
    "\n",
    "# Define models\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "\n",
    "\n",
    "# Create a Voting Classifier to combine both models\n",
    "voting_model = VotingClassifier(estimators=[\n",
    "    ('rf', rf_model),\n",
    "    ('xgb', xgb_model)\n",
    "], voting='soft')\n",
    "\n",
    "# Create a pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),      # Standardize features\n",
    "    ('voting_model', voting_model)     # Use the voting model (ensemble)\n",
    "])\n",
    "\n",
    "# Train the pipeline with the training data\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_val_pred = pipeline.predict(x_val)\n",
    "y_val_proba = pipeline.predict_proba(x_val)[:, 1]\n",
    "\n",
    "# Calculate metrics for validation data\n",
    "print(\"Validation Classification Report:\")\n",
    "print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nValidation Confusion Matrix:\")\n",
    "print(confusion_matrix(y_val, y_val_pred))\n",
    "\n",
    "# Calculate ROC-AUC for validation data\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "print(f\"Validation ROC-AUC Score: {roc_auc_val:.4f}\")\n",
    "\n",
    "# Plot ROC Curve for validation data\n",
    "fpr_val, tpr_val, thresholds_val = roc_curve(y_val, y_val_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_val, tpr_val, color='g', label=f'Validation ROC curve (area = {roc_auc_val:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random classifier line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('Validation ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred = pipeline.predict(x_test)\n",
    "y_proba = pipeline.predict_proba(x_test)[:, 1]\n",
    "\n",
    "# Calculate ROC-AUC score for the test data\n",
    "roc_auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"Test ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Plot ROC Curve for test data\n",
    "fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_proba)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr_test, tpr_test, color='b', label=f'Test ROC curve (area = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Random classifier line\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate (Recall)')\n",
    "plt.title('Test ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "print(\"Test Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Cross-validation ---\n",
    "# Set up StratifiedKFold for cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Define the model pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),      # Standardize features\n",
    "    ('voting_model', voting_model)     # Use the voting model (ensemble)\n",
    "])\n",
    "\n",
    "# Perform cross-validation using ROC AUC as the evaluation metric\n",
    "roc_auc_scorer = make_scorer(roc_auc_score)\n",
    "\n",
    "# Apply cross-validation and calculate the ROC AUC scores for each fold\n",
    "cv_scores = cross_val_score(pipeline, X_res, y_res, cv=cv, scoring=roc_auc_scorer)\n",
    "\n",
    "# Print out the cross-validation results\n",
    "print(\"Cross-validation ROC AUC scores: \", cv_scores)\n",
    "print(\"Mean ROC AUC score: \", cv_scores.mean())\n",
    "\n",
    "# --- Train the pipeline ---\n",
    "# Fit the pipeline first\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# --- Feature Importance from Random Forest ---\n",
    "# Access Random Forest model inside the Voting Classifier (after fitting)\n",
    "rf_model = pipeline.named_steps['voting_model'].named_estimators_['rf']\n",
    "\n",
    "# Extract feature importance\n",
    "rf_feature_importance = rf_model.feature_importances_\n",
    "\n",
    "# Feature names and importance as a dataframe\n",
    "feat_names = X.columns  # This should be defined before the feature importance step\n",
    "feat_importance_df = pd.DataFrame({\n",
    "    'Feature': feat_names,\n",
    "    'Importance': rf_feature_importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Show the top N features (e.g., top 20)\n",
    "top_n = 20\n",
    "top_features = feat_importance_df.head(top_n)\n",
    "\n",
    "# Visualize Random Forest feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='skyblue')  # Correcting the barh command\n",
    "plt.title(f'Top {top_n} Feature Importances (Random Forest)')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest importance on top\n",
    "plt.show()\n",
    "\n",
    "# --- Feature Importance from XGBoost using SHAP ---\n",
    "# Access XGBoost model inside the Voting Classifier (after fitting)\n",
    "xgb_model = pipeline.named_steps['voting_model'].named_estimators_['xgb']\n",
    "\n",
    "# Use SHAP for XGBoost feature importance\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(X_train)\n",
    "\n",
    "# Visualize SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_train, feature_names=feat_names)\n",
    "\n",
    "# --- Evaluate the Model ---\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# --- Classification Report ---\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# --- ROC-AUC Curve ---\n",
    "y_prob = pipeline.predict_proba(X_test)[:, 1]\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# --- Precision-Recall Curve ---\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()\n",
    "\n",
    "# --- Confusion Matrix ---\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Non-Bankrupt', 'Bankrupt'], yticklabels=['Non-Bankrupt', 'Bankrupt'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimising code for clarity and presentation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility Functions ---\n",
    "\n",
    "# --- Helper Function for Metrics and Plots ---\n",
    "def evaluate_model(pipeline, X, y, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Evaluate a trained pipeline on a dataset and display performance metrics.\n",
    "    Outputs classification report, confusion matrix, and ROC-AUC/PR curves.\n",
    "    \"\"\"\n",
    "    y_pred = pipeline.predict(X)\n",
    "    y_proba = pipeline.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"\\n{dataset_name} Classification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(f\"\\n{dataset_name} Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Bankrupt', 'Bankrupt'], \n",
    "                yticklabels=['Non-Bankrupt', 'Bankrupt'])\n",
    "    plt.title(f'{dataset_name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.title(f'{dataset_name} ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
    "    plt.title(f'{dataset_name} Precision-Recall Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\"roc_auc\": roc_auc, \"pr_auc\": pr_auc}\n",
    "\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, auc_score, title):\n",
    "    \"\"\"Plots ROC curve.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pr_curve(precision, recall, pr_auc, title):\n",
    "    \"\"\"Plots Precision-Recall curve.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(recall, precision, label=f'PR curve (AUC = {pr_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Features and target\n",
    "X = df.drop('Bankrupt?', axis=1)\n",
    "y = df['Bankrupt?']\n",
    "\n",
    "# Handle missing values\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X[X.abs() > 1e10] = np.nan  # Combine extreme outlier handling\n",
    "X_imputed = SimpleImputer(strategy='median').fit_transform(X)\n",
    "\n",
    "# SMOTE for oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_imputed, y)\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "x_temp, x_val, y_temp, y_val = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "\n",
    "# --- Model Definition ---\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "voting_model = VotingClassifier(estimators=[('rf', rf_model), ('xgb', xgb_model)], voting='soft')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('voting_model', voting_model)\n",
    "])\n",
    "\n",
    "# --- Training ---\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "# --- Validation ---\n",
    "y_val_proba = pipeline.predict_proba(x_val)[:, 1]\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "print(f\"Validation ROC-AUC Score: {roc_auc_val:.4f}\")\n",
    "\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_proba)\n",
    "plot_roc_curve(fpr_val, tpr_val, roc_auc_val, 'Validation ROC Curve')\n",
    "\n",
    "# --- Testing ---\n",
    "y_test_proba = pipeline.predict_proba(x_test)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "print(f\"Test ROC-AUC Score: {roc_auc_test:.4f}\")\n",
    "\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\n",
    "plot_roc_curve(fpr_test, tpr_test, roc_auc_test, 'Test ROC Curve')\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "plot_pr_curve(precision, recall, pr_auc, 'Test Precision-Recall Curve')\n",
    "\n",
    "# --- Feature Importance ---\n",
    "rf_model = pipeline.named_steps['voting_model'].named_estimators_['rf']\n",
    "feat_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feat_importance_df['Feature'][:20], feat_importance_df['Importance'][:20], color='skyblue')\n",
    "plt.title('Top 20 Feature Importances (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --- SHAP for XGBoost ---\n",
    "xgb_model = pipeline.named_steps['voting_model'].named_estimators_['xgb']\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(x_train)\n",
    "shap.summary_plot(shap_values, x_train, feature_names=X.columns)\n",
    "\n",
    "\n",
    "# --- Evaluate on Validation Set ---\n",
    "print(\"Validation Set Evaluation:\")\n",
    "validation_metrics = evaluate_model(pipeline, x_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "# --- Evaluate on Test Set ---\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_metrics = evaluate_model(pipeline, x_test, y_test, dataset_name=\"Test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Utility Functions ---\n",
    "\n",
    "# --- Helper Function for Metrics and Plots ---\n",
    "def evaluate_model(pipeline, X, y, dataset_name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Evaluate a trained pipeline on a dataset and display performance metrics.\n",
    "    Outputs classification report, confusion matrix, and ROC-AUC/PR curves.\n",
    "    \"\"\"\n",
    "    y_pred = pipeline.predict(X)\n",
    "    y_proba = pipeline.predict_proba(X)[:, 1]\n",
    "    \n",
    "    # Classification Report\n",
    "    print(f\"\\n{dataset_name} Classification Report:\")\n",
    "    print(classification_report(y, y_pred))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print(f\"\\n{dataset_name} Confusion Matrix:\")\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['Non-Bankrupt', 'Bankrupt'], \n",
    "                yticklabels=['Non-Bankrupt', 'Bankrupt'])\n",
    "    plt.title(f'{dataset_name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # ROC Curve\n",
    "    fpr, tpr, _ = roc_curve(y, y_proba)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "    plt.title(f'{dataset_name} ROC Curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    precision, recall, _ = precision_recall_curve(y, y_proba)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(recall, precision, color='blue', lw=2, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
    "    plt.title(f'{dataset_name} Precision-Recall Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\"roc_auc\": roc_auc, \"pr_auc\": pr_auc}\n",
    "\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, auc_score, title):\n",
    "    \"\"\"Plots ROC curve.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pr_curve(precision, recall, pr_auc, title):\n",
    "    \"\"\"Plots Precision-Recall curve.\"\"\"\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.plot(recall, precision, label=f'PR curve (AUC = {pr_auc:.2f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Data Preparation ---\n",
    "# Features and target\n",
    "X = df.drop('Bankrupt?', axis=1)\n",
    "y = df['Bankrupt?']\n",
    "\n",
    "# Handle missing values\n",
    "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "X[X.abs() > 1e10] = np.nan  # Combine extreme outlier handling\n",
    "X_imputed = SimpleImputer(strategy='median').fit_transform(X)\n",
    "\n",
    "# SMOTE for oversampling\n",
    "smote = SMOTE(random_state=42)\n",
    "X_res, y_res = smote.fit_resample(X_imputed, y)\n",
    "\n",
    "# Split into training, validation, and test sets\n",
    "x_temp, x_val, y_temp, y_val = train_test_split(X_res, y_res, test_size=0.2, random_state=42, stratify=y_res)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_temp, y_temp, test_size=0.25, random_state=42, stratify=y_temp)\n",
    "\n",
    "# --- Model Definition ---\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "voting_model = VotingClassifier(estimators=[('rf', rf_model), ('xgb', xgb_model)], voting='soft')\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()), \n",
    "    ('voting_model', voting_model)\n",
    "])\n",
    "\n",
    "# --- Cross-Validation Setup ---\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "\n",
    "# Define cross-validation strategy\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Evaluate pipeline using cross-validation\n",
    "cv_scores = cross_val_score(pipeline, X_res, y_res, scoring='roc_auc', cv=cv, n_jobs=-1)\n",
    "\n",
    "print(\"\\nCross-Validation ROC-AUC Scores:\")\n",
    "print(cv_scores)\n",
    "print(f\"Mean ROC-AUC: {np.mean(cv_scores):.4f}\")\n",
    "print(f\"Standard Deviation: {np.std(cv_scores):.4f}\")\n",
    "\n",
    "# --- Training and Final Evaluation on Test Set ---\n",
    "pipeline.fit(x_train, y_train)  # Retrain on full training set for final evaluation\n",
    "\n",
    "# --- Validation ---\n",
    "y_val_proba = pipeline.predict_proba(x_val)[:, 1]\n",
    "roc_auc_val = roc_auc_score(y_val, y_val_proba)\n",
    "print(f\"Validation ROC-AUC Score: {roc_auc_val:.4f}\")\n",
    "\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_proba)\n",
    "plot_roc_curve(fpr_val, tpr_val, roc_auc_val, 'Validation ROC Curve')\n",
    "\n",
    "# --- Testing ---\n",
    "y_test_proba = pipeline.predict_proba(x_test)[:, 1]\n",
    "roc_auc_test = roc_auc_score(y_test, y_test_proba)\n",
    "print(f\"Test ROC-AUC Score: {roc_auc_test:.4f}\")\n",
    "\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\n",
    "plot_roc_curve(fpr_test, tpr_test, roc_auc_test, 'Test ROC Curve')\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_test_proba)\n",
    "pr_auc = auc(recall, precision)\n",
    "plot_pr_curve(precision, recall, pr_auc, 'Test Precision-Recall Curve')\n",
    "\n",
    "# --- Feature Importance ---\n",
    "rf_model = pipeline.named_steps['voting_model'].named_estimators_['rf']\n",
    "feat_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(feat_importance_df['Feature'][:20], feat_importance_df['Importance'][:20], color='skyblue')\n",
    "plt.title('Top 20 Feature Importances (Random Forest)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# --- SHAP for XGBoost ---\n",
    "xgb_model = pipeline.named_steps['voting_model'].named_estimators_['xgb']\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "shap_values = explainer.shap_values(x_train)\n",
    "shap.summary_plot(shap_values, x_train, feature_names=X.columns)\n",
    "\n",
    "\n",
    "# --- Evaluate on Validation Set ---\n",
    "print(\"Validation Set Evaluation:\")\n",
    "validation_metrics = evaluate_model(pipeline, x_val, y_val, dataset_name=\"Validation\")\n",
    "\n",
    "# --- Evaluate on Test Set ---\n",
    "print(\"\\nTest Set Evaluation:\")\n",
    "test_metrics = evaluate_model(pipeline, x_test, y_test, dataset_name=\"Test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
