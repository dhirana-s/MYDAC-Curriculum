{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (0.12.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (from imbalanced-learn) (2.1.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (from imbalanced-learn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\desktop\\coding\\mydac-curriculum\\.venv\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#Goal: predict if a company goes bankrupt or not\n",
    "%pip install imbalanced-learn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import imbalanced-learn as imblearn\n",
    "from scipy.stats import kurtosis, skew\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sig no. of outliers observed for most columns - via dataWrangler. Since this is financial data, check for long-tailed distribution aka tail extends further than normal distribution. If there is no long-tailed distribution, we can apply Random Forest without log transformation to account for skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Skewness     Kurtosis\n",
      "Fixed Assets to Assets                   82.577237  6814.000147\n",
      "Current Ratio                            82.577237  6814.000147\n",
      "Total income/Total expense               82.332424  6786.903523\n",
      "Net Value Growth Rate                    80.291844  6540.116467\n",
      "Contingent liabilities/Net worth         79.670620  6487.125425\n",
      "Realized Sales Gross Profit Growth Rate  77.925109  6291.000429\n",
      "Continuous Net Profit Growth Rate        67.097534  5392.615103\n",
      "Total Asset Return Growth Rate Ratio     62.499961  5071.235869\n",
      "Revenue per person                       59.434480  3568.408258\n",
      "Quick Assets/Current Liability           47.947300  2305.178322\n"
     ]
    }
   ],
   "source": [
    "# # #This isn't working so gonna try a diff method results unreliable according to vscode due to many identical values\n",
    "# # Calculate skewness and kurtosis for each feature\n",
    "# skewness = df.skew()\n",
    "# kurtosis_values = df.apply(kurtosis)\n",
    "\n",
    "# # Print skewness and kurtosis summary\n",
    "# summary = pd.DataFrame({'Skewness': skewness, 'Kurtosis': kurtosis_values})\n",
    "# print(summary)\n",
    "\n",
    "# # Filter for skewed features (consider skewness > 1 or < -1 as skewed)\n",
    "# skewed_features = summary[summary['Skewness'].abs() > 1]\n",
    "# print(\"Skewed Features:\")\n",
    "# print(skewed_features)\n",
    "\n",
    "\n",
    "\n",
    "#Method 2: accounting for huge no. of columns\n",
    "df_non_constant = df.loc[:, df.nunique() > 1] #filter out constant/nearly constant features to avoid precision loss issue\n",
    "\n",
    "# Calculate skewness and kurtosis for each feature\n",
    "skewness = df_non_constant.skew()\n",
    "kurtosis_values = df_non_constant.apply(kurtosis)\n",
    "\n",
    "summary = pd.DataFrame({'Skewness': skewness, 'Kurtosis': kurtosis_values}) # Create a summary DataFrame with both skewness and kurtosis\n",
    "sorted_summary = summary.sort_values(by=['Skewness', 'Kurtosis'], ascending=False) # Sort by Skewness and Kurtosis in desc order (most problematic first)\n",
    "print(sorted_summary.head(10))  # Shows top 10 most problematic columns based on skewness and kurtosis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there's kurtosis in the 6000s (should be around 3) and skewness in 80s (should be around 1) its very high even for financial data from a good source cos it suggests heavy-tailed distributions far from normality and extreme outliers. could be outliers but i googled and saw the years in the data are the 1997 asian financial crisis, 2001 recession, 2008 global recession Taiwan faced. But tbh the high skewness in financial data makes sense cos of the nature of how assets and liabilities behave in real world aka right skewed distribution since small no. of firms will have most of market value/revenue. Also bankruptcies are rare events that the model must capture.\n",
    "\n",
    "So things to do:\n",
    "1. find the key features w extreme values/skewness by reviewing feature importance\n",
    "2. apply log transformation to features with high skewness\n",
    "3. use tree-based model random forst which handles outliers and skewed data better\n",
    "4. Keep in mind that your target variable (bankruptcy) is likely highly imbalanced (i.e., most companies won't go bankrupt). This imbalance itself can introduce skewness into your dataset. You may need to use techniques like SMOTE (Synthetic Minority Over-sampling Technique) or class weighting in your model to account for the imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9655\n",
      "Confusion Matrix:\n",
      "[[1310    3]\n",
      " [  44    7]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      1313\n",
      "           1       0.70      0.14      0.23        51\n",
      "\n",
      "    accuracy                           0.97      1364\n",
      "   macro avg       0.83      0.57      0.61      1364\n",
      "weighted avg       0.96      0.97      0.95      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df['Bankrupt?']\n",
    "x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler() #initialise scaler to bring it to normal distribution\n",
    "x_train_scaled = scaler.fit_transform(x_train) #fit scaler on training data and transform it\n",
    "x_test_scaled = scaler.transform(x_test) #transform test data using same scaler\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10, random_state=42) # Initialize the model\n",
    "model.fit(x_train_scaled, y_train) # Fit the model to the training data\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test_scaled) #get the trained model to make predictions on test set\n",
    "\n",
    "\n",
    "# Now we can evaluate how well the model did.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification report (precision, recall, f1-score)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So accuracy is noice. But a bit misleading since got very imbalanced classes (ie. lot more safe than bankrupty firms). The model is just predicting the majority safe class correctly.\n",
    "\n",
    "Problems:\n",
    "1. there are 44 firms predicted to be safe but were actually bankrupt.\n",
    "2. only 70% precision for class 1 - meaning only 70% of firms predicted to be bankrupt were actually bankrupt\n",
    "3. only 14% recall for class 1 - meaning only 14% of actually bankrupt firms were correctly predicted as bankrupt. very bad cos the model is failing to identify many bankrupt companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try again.\n",
    "#add in the class_weight='balanced' to the model and see how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9670\n",
      "Confusion Matrix:\n",
      "[[1309    4]\n",
      " [  41   10]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      1313\n",
      "           1       0.71      0.20      0.31        51\n",
      "\n",
      "    accuracy                           0.97      1364\n",
      "   macro avg       0.84      0.60      0.65      1364\n",
      "weighted avg       0.96      0.97      0.96      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y = df['Bankrupt?']\n",
    "x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "scaler = StandardScaler() #initialise scaler to bring it to normal distribution\n",
    "x_train_scaled = scaler.fit_transform(x_train) #fit scaler on training data and transform it\n",
    "x_test_scaled = scaler.transform(x_test) #transform test data using same scaler\n",
    "\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10, class_weight='balanced', random_state=42) # Initialize the model\n",
    "model.fit(x_train_scaled, y_train) # Fit the model to the training data\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test_scaled) #get the trained model to make predictions on test set\n",
    "\n",
    "\n",
    "# Now we can evaluate how well the model did.\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "# Confusion matrix\n",
    "print('Confusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Classification report (precision, recall, f1-score)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are small improvements noice. let's test out SMOTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1     169\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "After SMOTE: Bankrupt?\n",
      "0    5286\n",
      "1    5286\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Accuracy: 0.9567\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1277   36]\n",
      " [  23   28]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98      1313\n",
      "           1       0.44      0.55      0.49        51\n",
      "\n",
      "    accuracy                           0.96      1364\n",
      "   macro avg       0.71      0.76      0.73      1364\n",
      "weighted avg       0.96      0.96      0.96      1364\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# y = df['Bankrupt?']\n",
    "# x = df.drop('Bankrupt?', axis=1)\n",
    "\n",
    "# x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# scaler = StandardScaler() #initialise scaler to bring it to normal distribution\n",
    "# x_train_scaled = scaler.fit_transform(x_train) #fit scaler on training data and transform it\n",
    "# x_test_scaled = scaler.transform(x_test) #transform test data using same scaler\n",
    "\n",
    "\n",
    "# smote = SMOTE(random_state=42) #initialise SMOTE\n",
    "# x_train_smote, y_train_smote = smote.fit_resample(x_train_scaled, y_train)\n",
    "# print(\"Before SMOTE:\", y_train.value_counts())\n",
    "# print(\"\\n\\nAfter SMOTE:\", pd.Series(y_train_smote).value_counts())\n",
    "\n",
    "\n",
    "# model = RandomForestClassifier(n_estimators=100, random_state=42) # Initialize Random Forest\n",
    "# model.fit(x_train_smote, y_train_smote) # fit model to trained data\n",
    "\n",
    "# y_pred = model.predict(x_test_scaled) # make predictions on the test set\n",
    "\n",
    "# # Accuracy of model\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(f'\\nAccuracy: {accuracy:.4f}')\n",
    "\n",
    "# # Evaluate the model\n",
    "# print(\"\\nConfusion Matrix:\")\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# print(\"\\nClassification Report:\")\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok So when we did SMOTE, some things happened.\n",
    "\n",
    "1. The precision of class 1 dropped -> from 70% to only 44% of firms predicted to be bankrupt were actually bankrupt.\n",
    "2. Recall of class 1 increased -> from 14%, its not 55% of actually bankrupt firms being correctly predicted to be bankrupt. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
